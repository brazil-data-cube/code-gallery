{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDfBxippkmQa"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/logo-bdc.png\" align=\"right\" width=\"64\"/>\n",
        "\n",
        "# <span style=\"color:#336699\">Brazil Data Cube Platform: Earth Observation data cubes and satellite image time series analysis</span>\n",
        "<hr style=\"border:2px solid #0077b9;\">\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"text-align: center;font-size: 90%;\">\n",
        "    Karine R. Ferreira, Gilberto R. Queiroz, Baggio L. C. Silva, Fabiana Ziotti, Raphael W. Costa, Rennan F. B. Marujo, Gabriel Sansigolo\n",
        "    <br/><br/>\n",
        "    Earth Observation and Geoinformatics Division, National Institute for Space Research (INPE)\n",
        "    <br/>\n",
        "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
        "    <br/><br/>\n",
        "    Contact: <a href=\"mailto:brazildatacube@inpe.br\">brazildatacube@inpe.br</a>\n",
        "    <br/><br/>\n",
        "    Last Update: Nov 01, 2024\n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
        "<b>Abstract.</b> This Jupyter Notebook gives an overview on some of the <em>Brazil Data Cube</em> services. Initially this notebooks illustrates a land cover classification by a traditional approach, which uses a single image in time to perform suppervised classification. Than, it is examplified the data cube approach, using the SpatioTemporal Asset Catalog (STAC) service to discover available images, the Web Time Series Service (WTSS) to extract time series, we run some examples, as cloud filtering, than, the Web Land Trajectory Service (WLTS) to analyse samples in different mapping projects and finally, the temporal information from the time series is used in a classification.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-cxGPDQI_rU"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/logo-bdc.png\" align=\"right\" width=\"64\"/>\n",
        "\n",
        "# Brazil Data Cube\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTvNSzs-JLVa"
      },
      "source": [
        "Remote sensing traditionally relied on single-scene approaches for classification. However, this practice has evolved significantly in recent years due to the high availability of data from various satellites and sensors and the processing capabilities of modern computers.\n",
        "\n",
        "The use of time series in remote sensing has revolutionized the field by providing a dynamic and comprehensive perspective on environmental changes and land use patterns. Time series analysis involves the analysys of successive time intervals, allowing for the detection of trends, seasonal variations, and anomalies over time.\n",
        "\n",
        "The volume of big data present both opportunities and challenges. On one hand, the availability of extensive datasets allows for more accurate and nuanced analyses. On the other hand, managing, processing, and analyzing such large datasets require advanced computational techniques and significant resources.\n",
        "\n",
        "In this context, Earth Observation (EO) datacubes represent a powerful innovation in the field, offering a structured and multidimensional way to store, manage, and analyze large volumes of EO data. EO datacubes organize data in a three-dimensional grid, with spatial dimensions (latitude and longitude) and a temporal dimension (time), allowing for efficient and systematic access to time series data across various locations.\n",
        "\n",
        "Brazil Data Cube (BDC) is a research, development and technological innovation project of the National Institute for Space Research (INPE), Brazil. It is producing data sets from big volumes of medium-resolution remote sensing images for the entire national territory and developing a computational platform to process and analyze these data sets using artificial intelligence, machine learning and image time series analysis.\n",
        "\n",
        "The data sets produced in the BDC project include collections of analysis-ready data (ARD), multidimensional data cubes and mosaics from images of the CBERS-4/4A, Sentinel-2 and Landsat-8/9 satellites. The computational platform is composed of web services, software applications and iterative computing environments. Using artificial intelligence, machine learning, and image time series analysis, land use and land cover maps are being produced from these data cubes.\n",
        "\n",
        "For more information on Brazil Data Cube, please see:\n",
        "\n",
        "<a href=\"https://data.inpe.br/bdc/web/en/home-page-2/\">BDC Website</a>\n",
        "\n",
        "<a href=\"https://github.com/brazil-data-cube\">BDC GitHub</a>\n",
        "\n",
        "<a href=\"https://brazil-data-cube.github.io/\">BDC GitHub IO</a>\n",
        "\n",
        "<a href=\"https://data.inpe.br/stac/browser/?.language=en\">INPE STAC Browser</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LebxhudhkmQf"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/stac/stac.png?raw=true\" align=\"right\" width=\"66\"/>\n",
        "\n",
        "# **S**patio**T**emporal **A**sset **C**atalog (STAC)\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZn7b78KPQDG"
      },
      "source": [
        "The [**S**patio**T**emporal **A**sset **C**atalog (STAC)](https://stacspec.org/) is a specification created through the colaboration of several organizations intended to increase satellite image search interoperability.\n",
        "\n",
        "The diagram depicted in the picture contains the most important concepts behind the STAC data model:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/stac/stac-concept.png\" width=\"480\" />\n",
        "<br/>\n",
        "STAC model.\n",
        "</center>\n",
        "\n",
        "The description of the concepts below are adapted from the [STAC Specification](https://github.com/radiantearth/stac-spec):\n",
        "\n",
        "- **Item**: a `STAC Item` is the atomic unit of metadata in STAC, providing links to the actual `assets` (including thumbnails) that they represent. It is a `GeoJSON Feature` with additional fields for things like time, links to related entities and mainly to the assets. According to the specification, this is the atomic unit that describes the data to be discovered in a `STAC Catalog` or `Collection`.\n",
        "\n",
        "- **Asset**: a `spatiotemporal asset` is any file that represents information about the earth captured in a certain space and time.\n",
        "\n",
        "\n",
        "- **Catalog**: provides a structure to link various `STAC Items` together or even to other `STAC Catalogs` or `Collections`.\n",
        "\n",
        "\n",
        "- **Collection:** is a specialization of the `Catalog` that allows additional information about a spatio-temporal collection of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLOjeOtkmQg"
      },
      "source": [
        "STAC Client API\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "For running the examples in this Jupyter Notebook you will need to install the [pystac-client](https://pystac-client.readthedocs.io/en/latest/). To install it from PyPI using `pip`, use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNOgCEuxkmQh",
        "outputId": "2bd326be-5172-4b66-8f70-a05b9d681833"
      },
      "outputs": [],
      "source": [
        "!pip install pystac-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG89dwJPQDM",
        "outputId": "d5c671b1-440c-4c81-f829-81dad219f2be"
      },
      "outputs": [],
      "source": [
        "!pip install shapely tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikBncksbkmQi"
      },
      "source": [
        "In order to access the funcionalities of the client API, you should import the `stac` package, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbh1DWTaPQDY"
      },
      "outputs": [],
      "source": [
        "import pystac_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5LicjUHkmQk"
      },
      "source": [
        "Then, create a `STAC` object attached to the Brazil Data Cube' STAC service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlODPqhtkmQk"
      },
      "outputs": [],
      "source": [
        "service = pystac_client.Client.open('https://data.inpe.br/bdc/stac/v1/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlsjmWuvkmQk"
      },
      "source": [
        "Listing the Available Data Products\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdmH2H7VkmQl"
      },
      "source": [
        "In the Jupyter environment, the `STAC` object will list the available image and data cube collections from the service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IKREEUpPQDg",
        "outputId": "046c18f9-1f56-4dc4-df29-6999339a449a"
      },
      "outputs": [],
      "source": [
        "for collection in service.get_collections():\n",
        "    print(collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yiuPB2YkmQl"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/stac/stac-catalog.png?raw=true\" align=\"right\" width=\"300\"/>\n",
        "\n",
        "Retrieving the Metadata of a Collection\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "The `collection` method returns information about a given image or data cube collection identified by its name. In this example we are retrieving information about the datacube collection `S2-16D-2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "dA0RhGdSPQDn",
        "outputId": "c436b0a5-a6c8-4e4a-b60a-9afa96b9135b"
      },
      "outputs": [],
      "source": [
        "collection = service.get_collection('S2-16D-2')\n",
        "collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoajiPikkmQm"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/stac/stac-item.png?raw=true\" align=\"right\" width=\"300\"/>\n",
        "\n",
        "Retrieving Items\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "The `get_items` method returns the items of a given collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sycz0XtI3dJN"
      },
      "outputs": [],
      "source": [
        "import folium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-jp_ReN3vgY"
      },
      "outputs": [],
      "source": [
        "bbox = [-52.3625, -6.43, -52.3575, -6.425]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "DBeMmM-A3txx",
        "outputId": "195495f9-501e-45be-a9d8-8a186a8f8f52"
      },
      "outputs": [],
      "source": [
        "f = folium.Figure(width=1000, height=300) # Restrict figure size\n",
        "\n",
        "# Create a folium map centered around the geographic area of interest\n",
        "folium_map = folium.Map(location=[-6.41, -52.35], zoom_start=13)\n",
        "\n",
        "folium.Rectangle(\n",
        "    bounds=[[bbox[1],bbox[0]],[bbox[3],bbox[2]]],\n",
        "    color=\"blue\",\n",
        "    weight=2,\n",
        "    fill=True,\n",
        "    fill_color=\"blue\",\n",
        "    fill_opacity=0.2\n",
        ").add_to(folium_map)\n",
        "\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLfQlYLzkmQm"
      },
      "source": [
        "In order to support filtering rules through the specification of a rectangle (`bbox`) or a date and time (`datatime`) criterias, use the `Client.search(**kwargs)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hCWnHXePQDt",
        "outputId": "7a9f4a49-2fe3-49d2-f31d-49be8007dd63",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "item_search = service.search(bbox=bbox,\n",
        "                             datetime='2020-01-01/2020-12-31',\n",
        "                             collections=['S2-16D-2'])\n",
        "item_search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZOgKrVgkmQn"
      },
      "source": [
        "The method `.search(**kwargs)` returns a `ItemSearch` representation which has handy methods to identify the matched results. For example, to check the number of items matched, use `.matched()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKpjaaFjkmQn",
        "outputId": "8c0e1e95-397d-4b45-cb15-96cf938408c3"
      },
      "outputs": [],
      "source": [
        "item_search.matched()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s36z4MrxkmQn"
      },
      "source": [
        "To iterate over the matched result, use `.get_items()` to traverse the list of items:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mwctoR0kmQn",
        "outputId": "3f0575d9-8101-4ce7-f170-f6dd98ef85e9"
      },
      "outputs": [],
      "source": [
        "for item in item_search.items():\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfBl6AsU5nDe",
        "outputId": "5115019d-d305-4b62-b190-f52785dd7d16"
      },
      "outputs": [],
      "source": [
        "items = list(item_search.items())\n",
        "items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgtiHNedkmQo"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/stac/stac-asset.png?raw=true\" align=\"right\" width=\"300\"/>\n",
        "\n",
        "Assets\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "The assets with the links to the images, thumbnails or specific metadata files, can be accessed through the property `assets` (from a given item):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyimexxXPQD1",
        "outputId": "1bd369fb-8ee7-4f34-db5d-b7d87fce588d"
      },
      "outputs": [],
      "source": [
        "assets = item.assets #Last item of the loop\n",
        "assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D4g3AO6kmQo"
      },
      "source": [
        "Then, from the assets it is possible to traverse or access individual elements:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gki8J5cqkmQp"
      },
      "source": [
        "The metadata related to the Sentinel-2/MSI blue band is available under the dictionary key `B02`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "2TifKb4ikmQp",
        "outputId": "5012ecaa-9663-47dd-8fb9-64acf167a645"
      },
      "outputs": [],
      "source": [
        "blue_asset = assets['B02']\n",
        "blue_asset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb76OEoWkmQp"
      },
      "source": [
        "To iterate in the item's assets, use the following pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSQHq_HEkmQp",
        "outputId": "61d32e00-d9d7-4e73-bf34-b9930b5d1a0d"
      },
      "outputs": [],
      "source": [
        "for asset in assets.values():\n",
        "    print(asset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yudqYLHMkmQ4"
      },
      "source": [
        "Retrieving Image Files\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Msl6F_kmQ5"
      },
      "source": [
        "Note that the URL for a given asset can be retrieved by the property `href`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lxsQCCcHkmQ5",
        "outputId": "f79bd0fc-70bb-4649-97de-83cb065426b7"
      },
      "outputs": [],
      "source": [
        "blue_asset.href"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQrgXwXP2OX4",
        "outputId": "10076cd7-9ac7-45d7-cb4b-bd00d3d06884"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3OSDTZ0oFhU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from pyproj import Transformer\n",
        "from pyproj.crs import CRS\n",
        "from rasterio.windows import bounds, from_bounds, Window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI2VEBOBoMOg"
      },
      "source": [
        "DataCubes generated by Brazil Data Cube use an Alber Equal Areas Projection ([see here](https://brazil-data-cube.github.io/specifications/bdc-projection.html)).\n",
        "\n",
        "Here we define some auxiliar functions to help in this Jupyter Notebook.\n",
        "\n",
        "- `normalize`: Normalizes image values (for visualization).\n",
        "\n",
        "- `read_img`: Reads an image using window.\n",
        "\n",
        "- `read_bdcimg_using_window_from_4326`: Reads parts (windows) of a BDC image using coordinates from EPSG 4326.\n",
        "\n",
        "- `reproj_with_transform`: Reprojects a pair of EPSG 4326 (lat lon) coordinates to the projection used by BDC.\n",
        "\n",
        "- `extract_samples`: extract the pixel values of a stack of images, given a set of coordinates pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crxdyPPCrJyp"
      },
      "outputs": [],
      "source": [
        "def normalize(array):\n",
        "    \"\"\"Normalizes numpy arrays into scale 0.0 - 1.0\"\"\"\n",
        "    array_min, array_max = array.min(), array.max()\n",
        "    return ((array - array_min)/(array_max - array_min))\n",
        "\n",
        "def read_img(uri: str, window: Window = None, masked: bool = True):\n",
        "    \"\"\"Read raster window as numpy.ma.masked_array.\"\"\"\n",
        "    with rasterio.open(uri) as src:\n",
        "        return src.read(1, window=window, masked=masked)\n",
        "\n",
        "def read_bdcimg_using_window_from_4326(uri: str, bbox, transformer):\n",
        "    \"\"\"Read raster window as numpy using EPSG:4326 to crop the window.\"\"\"\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    x1_reproj, y1_reproj = transformer.transform(x1, y1)\n",
        "    x2_reproj, y2_reproj = transformer.transform(x2, y2)\n",
        "    with rasterio.open(uri) as src:\n",
        "        window = from_bounds(x1_reproj, y1_reproj, x2_reproj, y2_reproj, src.transform)\n",
        "        rst = src.read(1, window=window)\n",
        "        window_transform = src.window_transform(window)\n",
        "        # window_bounds = bounds(window, src.transform)\n",
        "    return rst, window_transform\n",
        "\n",
        "def reproj_with_transform(coord, transformer):\n",
        "    \"\"\"Reprojects a pair of coordinates using a transform\"\"\"\n",
        "    x, y = coord\n",
        "    x_reproj, y_reproj = transformer.transform(x, y)\n",
        "    return x_reproj, y_reproj\n",
        "\n",
        "def extract_samples(band_stack, sample_coords, transformer, transform):\n",
        "    \"\"\"Extract pixel value of points\"\"\"\n",
        "    # Reproject the sample coordinates\n",
        "    reprojected_samples = [reproj_with_transform(sample, transformer) for sample in sample_coords]\n",
        "\n",
        "    extracted_values = []\n",
        "    for x_reproj, y_reproj in reprojected_samples:\n",
        "        # Get the row and column indices of the sample in the raster\n",
        "        row, col = ~transform * (x_reproj, y_reproj)\n",
        "        row, col = int(row), int(col)\n",
        "\n",
        "        # Ensure the row and column are within the bounds of the raster\n",
        "        if 0 <= row < band_stack.shape[0] and 0 <= col < band_stack.shape[1]:\n",
        "            # Extract the pixel values from the band stack at the given row and column\n",
        "            sample_values = band_stack[row, col, :]\n",
        "            extracted_values.append(sample_values)\n",
        "        else:\n",
        "            extracted_values.append([np.nan, np.nan, np.nan])  # Handle out-of-bounds appropriately\n",
        "\n",
        "    return np.array(extracted_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCZHcxBblxVd"
      },
      "source": [
        "Now let's suppose we don't want to use the entire image, only a part of it.\n",
        "\n",
        "So we define a bounding box of the area of interest in order to open and visualize the RGB bands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6zOtHsrshsv"
      },
      "outputs": [],
      "source": [
        "window_bbox = [-52.4, -6.5, -52.3, -6.4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87nABQza5ezd"
      },
      "outputs": [],
      "source": [
        "# Create the transformer\n",
        "crs = rasterio.open(assets['B02'].href).crs\n",
        "in_proj = CRS.from_epsg(4326)\n",
        "out_proj = CRS.from_user_input(crs)\n",
        "transformer = Transformer.from_crs(in_proj, out_proj, always_xy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "yXkrwOI2DBNP",
        "outputId": "d777c209-0e22-4273-bdea-fc998da9923a"
      },
      "outputs": [],
      "source": [
        "b02_image, window_transform = read_bdcimg_using_window_from_4326(items[7].assets['B02'].href, window_bbox, transformer)\n",
        "b03_image, _ = read_bdcimg_using_window_from_4326(items[7].assets['B03'].href, window_bbox, transformer)\n",
        "b04_image, _ = read_bdcimg_using_window_from_4326(items[7].assets['B04'].href, window_bbox, transformer)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
        "ax1.imshow(b02_image, cmap='gray')\n",
        "ax2.imshow(b03_image, cmap='gray')\n",
        "ax3.imshow(b04_image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "SqW94gNKBGUt",
        "outputId": "a2657992-cfa5-4220-dbd0-ba6005a5f12f"
      },
      "outputs": [],
      "source": [
        "rgb_normalized_stack = np.dstack((normalize(b04_image), normalize(b03_image), normalize(b02_image)))\n",
        "plt.imshow(rgb_normalized_stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYleM4lvpqw0"
      },
      "source": [
        "# <span style=\"color:#336699\">Web Land Trajectory Service (WLTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jN_v077HjXj"
      },
      "source": [
        "## <span style=\"color:#336699\">Introduction to WLTS\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3YuolIapkv8"
      },
      "source": [
        "The **W**eb **L**and **T**rajectory **S**ervice (WLTS) is a web service designed to access and retrieve trajectories of land use and coverage from different type of data sources. Through a simple API, it brings the concept of Land Use and Cover Trajectories as a high level abstraction. Given a location and a time interval you can retrieve the land trajectory from many data collections, including information from the PRODES, DETER, and TerraClass projects.\n",
        "\n",
        "`Figure 1` shows an example of representation of land use and cover trajectories extracted from a set of classified images, temporally ordered:\n",
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wlts/trajectory_def.png\" width=\"600\" />,\n",
        "    <br/>\n",
        "    <b>Figure 1</b> - Land use and cover Trajectory.\n",
        "</center>\n",
        "\n",
        "The WLTS introduces the following concepts:\n",
        "\n",
        "- **Collections**: refers to a specific dataset from a given data source. A collection can be either represented by vector or raster structures. It has a time interval delimited by time (tmin, tmax). In this way, each Collection has an associated time attribute, which is aligned according to the time granularity of each project that makes the Collection available.\n",
        "\n",
        "- **Class**: It is the label associated with a particular data item, which corresponds to the specific types of land use or cover, defined by the data source classification system. A Collection consists of a set of Class.\n",
        "\n",
        "- **Trajectory**: Given a spatial location (x, y), a trajectory is represented by a set of observations that contains the land use and land cover class, the name of collection and time associated with an x, y location in space.\n",
        "\n",
        "WLTS is based on three operations:\n",
        "\n",
        "- ``list_collections``: returns the list of collections available in the service.\n",
        "\n",
        "- ``describe_collection``: returns the metadata of a given data collection.\n",
        "\n",
        "- ``trajectory``: returns the land use and cover trajectory from the collections given a location in space. The property result contains the feature identifier information, class, time, and the collection associated to the data item.\n",
        "\n",
        "This Jupyter Notebook shows how to use the [Python Client Library](https://github.com/brazil-data-cube/wlts.py) for Web Land Trajectory Service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C764ouPkpzzw"
      },
      "source": [
        "Python Client API\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FW1oJep3Ca"
      },
      "source": [
        "For running the examples in this Jupyter Notebook you will need to install the [WLTS client for Python](https://github.com/brazil-data-cube/wlts.py).To install it from PyPI using pip, use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYoMWhOyp4wI",
        "outputId": "d5e01cde-759f-42b3-8da8-92051c8c7924"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/brazil-data-cube/wlts.py@v1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28m1sMYkpcRf"
      },
      "outputs": [],
      "source": [
        "import wlts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4L4gFDJqA1V"
      },
      "source": [
        "WLTS is a client-server service. On the server-side, the data is stored, which is accessible through each of the API operations, described earlier. On the client-side (what this tutorial covers), you can use the operations and consume the data. In this tutorial, we will use the Python client to access the data. We need to define the URL where the WLTS server is operating. The code below defines the URL of the WLTS server. You should create a wlts object attached to a given service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9DGgkUoqBFM"
      },
      "outputs": [],
      "source": [
        "service = wlts.WLTS('https://data.inpe.br/bdc/wlts/v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--UCzetzqHp_"
      },
      "source": [
        "Listing the Available Collections\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8YR5SxbqJg5"
      },
      "source": [
        "In WLTS, datasets that aggregate features from different classification systems, which various projects can generate, are represented through collections. Thus, the first operation presented is `list_collections`. This operation returns the list of all data collections that are available in the WLTS. In the  WLTS client for Python, this operation is used via the ``list_collections`` method which return a list of collection names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUgXabZTqLBS",
        "outputId": "21018669-d04b-4d4d-e73e-161126c91135"
      },
      "outputs": [],
      "source": [
        "service.collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NpJxLT_qYIO"
      },
      "source": [
        "Retrieving the Metadata of a collection\n",
        "<hr style=\"border:1px solid #0077b9;\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-v5iF4MqanA"
      },
      "source": [
        "Each collection is associated with a set of metadata that describes it. In WLTS a, there is the ``describe_collection`` operation, which allows the retrieval of this information. It is possible to access the metadata of a specific collection with the `operator[]`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oM33qllrqdK_",
        "outputId": "7e7e957e-b2fd-448e-9167-332dac41e71d"
      },
      "outputs": [],
      "source": [
        "service['terraclass_amazonia']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgaqXQZuqfrv"
      },
      "source": [
        "Retrieving the Trajectory\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn2_-FsUqhaG"
      },
      "source": [
        "In WLTS, since a collection is associated with a dataset with time variation, it is possible to retrieve the land use and land cover trajectory of a given point. The figure below illustrates this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mQe-Evmqj3Z"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wlts/traj1.png\" width=\"750\" />,\n",
        "    <br/>\n",
        "    <b>Figure 2</b> - WLTS trajectory extraction.\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMYtbYQqmKc"
      },
      "source": [
        "In order to retrieve the trajectory in the location of `latitude -12.0` and `longitude -54.0` use the `tj` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpGidgMcqn8p"
      },
      "outputs": [],
      "source": [
        "tj = service.tj(latitude=-12.0, longitude=-54.0, collections='mapbiomas-v8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve-4LVr3qyIj"
      },
      "source": [
        "WLTS allows more than one collection to be accessed at the same time for the same point. By doing this, a trajectory for each project will be extracted. This way of operation is illustrated by the figure below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmWH3BYdqz4I"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wlts/traj2.png\" width=\"750\" />,\n",
        "    <br/>\n",
        "    <b>Figure 3</b> - WLTS trajectory extraction using multiple collections.\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYBx64Z8q3BF"
      },
      "source": [
        "The names are entered in the collections parameter and must be separated by a comma. As an example, the code below retrieves the trajectories considering the collections ``mapbiomas_amazonia-v5`` and ``terraclass_amazonia.``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o6ME1J71qxX1",
        "outputId": "73f4c15f-1cc5-44bc-bce1-264af797cf61"
      },
      "outputs": [],
      "source": [
        "tj_multiples_collections = service.tj(latitude=-12.0, longitude=-54.0, collections='mapbiomas-v8,terraclass_amazonia')\n",
        "tj_multiples_collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtRazAwYrFBC"
      },
      "source": [
        "It is possible to retrieve the land use and land cover trajectory of a multiples point. The code below illustrates this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "HJR6ORDKrFJS",
        "outputId": "df8e6e4c-4d38-4e97-d954-974844378a6c"
      },
      "outputs": [],
      "source": [
        "tj_m = service.tj(latitude=[-8.485646, -12.0], longitude=[-56.869833, -54.0], collections='mapbiomas-v8')\n",
        "tj_m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys2pxk17rKmn"
      },
      "source": [
        "If you have Pandas installed, it is possible to plot the trajectory with the `df` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9I210cfxrJKT",
        "outputId": "48a4fbed-f6e7-4c11-dcba-7ed51f7255af"
      },
      "outputs": [],
      "source": [
        "tj.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcBN8xRNh_Wu"
      },
      "source": [
        "## Obtaining Samples\n",
        "\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MABlWNu3jiEp"
      },
      "source": [
        "When we have a historic of images, we can use then for extracting time series.\n",
        "\n",
        "This type of analysis allows the extraction of information presented over time that can be not seen when considering only the space.\n",
        "\n",
        "Time series can be used to identify changes in trends, as well as cycles. For agricultural areas, it can be used to extract several phenological metrics.\n",
        "Let's try a simple time series approach. First we will obtain the series from a set of points, we will calculate a spectral index. Obtain the class of each point given an existing project (IBGE and MapBiomas) and will perform a simple classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcVGIZ8iGNr"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://e-sensing.github.io/sitsbook/images/sits_general_view.png\" width=\"500\">\n",
        "\n",
        "Example of Land use and Land Cover mapping workflow through time series approach ([SITS, 2024](https://e-sensing.github.io/sitsbook/introduction.html)).\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJdX8T_OMayc"
      },
      "outputs": [],
      "source": [
        "import geopandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDodYVp3Q450"
      },
      "source": [
        "Let's start by reading a shapefile.\n",
        "\n",
        "You can open the shapefile directly from the .zip in your computer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozei2oLwQB5U"
      },
      "outputs": [],
      "source": [
        "# zipfile = \"sao-felix-do-xingu_utm_sqr_pts1km_subset80.zip\"\n",
        "# samples_df = geopandas.read_file(zipfile)\n",
        "# samples_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUSYH1y_Q92h"
      },
      "source": [
        "You can also obtain it from an url:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "cYtrFkcVMh0E",
        "outputId": "cd6984af-cfb1-43ce-f871-8d5668344c2e"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import zipfile\n",
        "\n",
        "zipfile_url = \"https://github.com/brazil-data-cube/code-gallery/raw/master/jupyter/Data/wlts/sao-felix-do-xingu_utm_sqr_pts1km_subset80.zip\"\n",
        "response = requests.get(zipfile_url)\n",
        "with tempfile.TemporaryDirectory() as tmpdir:\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "        z.extractall(tmpdir)\n",
        "\n",
        "        shp_file = [f for f in os.listdir(tmpdir) if f.endswith('.shp')][0]\n",
        "        shp_path = os.path.join(tmpdir, shp_file)\n",
        "\n",
        "        samples_df = geopandas.read_file(shp_path)\n",
        "\n",
        "samples_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdYW5ud5xHHh"
      },
      "source": [
        "We can see how many points we have by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HImivP9YrVI8",
        "outputId": "5902c052-6e46-4dbb-fa1e-2131c9cd84d2"
      },
      "outputs": [],
      "source": [
        "len(samples_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxUkdKJKxKnU"
      },
      "source": [
        "Now let's visualize the points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "pJzGA8o_cQ5a",
        "outputId": "d0642ff7-5e5e-4fcf-ef36-c48aace507d3"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from shapely.geometry import Point, MultiPoint\n",
        "\n",
        "f = folium.Figure(width=1000, height=300) # Restrict figure size\n",
        "\n",
        "# Create a folium map centered around the geographic area of interest\n",
        "folium_map = folium.Map(location=[-6.41, -52.35], zoom_start=13)\n",
        "\n",
        "for _, row in samples_df.iterrows():\n",
        "    point = row['geometry']\n",
        "    folium.CircleMarker(\n",
        "        location=(point.y, point.x),\n",
        "        radius=2,\n",
        "        color='gray',\n",
        "        fill=True,\n",
        "        fill_color='gray',\n",
        "        fill_opacity=0.5,\n",
        "        popup='Sample'\n",
        "    ).add_to(folium_map)\n",
        "\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHw4vLSioc3K"
      },
      "source": [
        "Obtain point labels using WLTS\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqn0VffoozsM"
      },
      "outputs": [],
      "source": [
        "#pip install git+https://github.com/brazil-data-cube/wlts.py@v1.0.1\n",
        "import wlts\n",
        "service = wlts.WLTS('https://data.inpe.br/bdc/wlts/v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTGkzJY7qYlA"
      },
      "source": [
        "**IBGE - Monitoramento e uso da Terra (2020)**\n",
        "\n",
        "In WLTS, the collection with IBGE data from the Land Use Monitoring project is in the collection named `ibge_land_use_cover`. The code below extracts the label of this collection in the year 2020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eoLE0-tPo6rN",
        "outputId": "d9c421ec-e6f5-47ca-9979-381671004a2d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_ibge = []\n",
        "\n",
        "# Extract classes with WLTS\n",
        "for point_row in samples_df.iterrows():\n",
        "    point_row = point_row[1]\n",
        "\n",
        "    ibge_class = service.tj(latitude  = float(point_row.geometry.y),\n",
        "                            longitude = float(point_row.geometry.x),\n",
        "                            start_date = 2020, end_date = 2020,\n",
        "                            collections = \"ibge_cobertura_uso_terra\")\n",
        "\n",
        "    samples_ibge.append(ibge_class.df())\n",
        "\n",
        "# Create a Data Frame\n",
        "samples_ibge = pd.concat(samples_ibge).reset_index(drop=True)\n",
        "samples_ibge[\"geometry\"] = samples_df[\"geometry\"]\n",
        "samples_ibge.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsnhcRvYqeJB"
      },
      "source": [
        "Analogous to the IBGE data, this section extracts the data from MapBiomas. In WLTS, the data from MapBiomas (Version 8) are represented through the collection `mapbiomas-v8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zbETfI1OpCvm",
        "outputId": "e9efb893-3c4b-41cd-901d-300e5c5147d7"
      },
      "outputs": [],
      "source": [
        "samples_mapbiomas = []\n",
        "\n",
        "# Extract classes with WLTS\n",
        "for point_row in samples_df.iterrows():\n",
        "    point_row = point_row[1]\n",
        "\n",
        "    mapbiomas_class = service.tj(latitude  = float(point_row.geometry.y),\n",
        "                                 longitude = float(point_row.geometry.x),\n",
        "                                 start_date = 2020,\n",
        "                                 end_date = 2020,\n",
        "                                 collections = \"mapbiomas-v8\")\n",
        "\n",
        "    samples_mapbiomas.append(mapbiomas_class.df())\n",
        "\n",
        "# Create a Data Frame\n",
        "samples_mapbiomas = pd.concat(samples_mapbiomas).reset_index(drop=True)\n",
        "samples_mapbiomas[\"geometry\"] = samples_df[\"geometry\"]\n",
        "samples_mapbiomas.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sDWiywIqrpo"
      },
      "source": [
        "### Prepare data to Water concordance analysis\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n4N8ggkqxc9"
      },
      "source": [
        "This section prepares the data for the concordance analysis. In this process, all points identified as water have their path values converted to `1`, while all other values are represented by `0`.\n",
        "\n",
        "This conversion is applied considering that there is one class that represents the Water element for each collection. The table below summarizes how each collection does this representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9d0esUUq2yk"
      },
      "source": [
        "|       **Collection**       | **Nomenclature for Water Class**   |\n",
        "|:--------------------------:|:----------------------------------:|\n",
        "|        IBGE (2020)         | Corpo d'água Continental           |\n",
        "| MapBiomas Versão 8 (2020)  | Rio, Lago e Oceano                 |\n",
        "\n",
        "\n",
        "Considering the information in the table, below each of the collections is prepared for classification.\n",
        "\n",
        "**IBGE Collection (2020)**\n",
        "> After running the command below, notice that the `class` column has its value summed to the values `0` and `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "8t6ieOHcrLX8",
        "outputId": "3cbb5603-6ad6-4d38-c2ed-0e2a2f6d7f17"
      },
      "outputs": [],
      "source": [
        "samples_ibge.loc[samples_ibge[\"class\"] != \"Corpo d'água Continental\", \"is_water\"] = False\n",
        "samples_ibge.loc[samples_ibge[\"class\"] == \"Corpo d'água Continental\", \"is_water\"] = True\n",
        "samples_ibge.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ-W7Zw8t3jG"
      },
      "source": [
        "**MapBiomas Collection (2020)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Ocq_TkwxrRhS",
        "outputId": "3cde5e80-a98b-4715-8600-0b0177090c4f"
      },
      "outputs": [],
      "source": [
        "samples_mapbiomas.loc[samples_mapbiomas[\"class\"] != \"Rio, Lago e Oceano\", \"is_water\"] = False\n",
        "samples_mapbiomas.loc[samples_mapbiomas[\"class\"] == \"Rio, Lago e Oceano\", \"is_water\"] = True\n",
        "samples_mapbiomas.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMeaV-66vD46"
      },
      "source": [
        "### Concordance analysis\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk01PvIHvR7r"
      },
      "source": [
        "Below we will generate an example of a concordance analysis. A confusion matrix is generated to visualize and quantify the points that have a concordance. After visualizing the matrix, the data is filtered so that only the points where there is concordance are considered.\n",
        "\n",
        "> **Note**: The analysis below does not consider many of the practical complexities involved in this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJfso2bjvWMq"
      },
      "outputs": [],
      "source": [
        "import seaborn\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okqtLBlEvYkQ"
      },
      "outputs": [],
      "source": [
        "# generate the confusion matrix\n",
        "\n",
        "cm_arr = confusion_matrix(samples_ibge[\"is_water\"].astype(\"int\"), samples_mapbiomas[\"is_water\"].astype(\"int\"))\n",
        "\n",
        "# formating results\n",
        "reference = [\"Non-Water\", \"Water\"]\n",
        "\n",
        "cm_pd = pd.DataFrame(cm_arr, columns = reference, index = reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ZO4X7apWvZ-5",
        "outputId": "dc6711b7-da87-4fb0-efbb-c8f5eb217c6e"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(dpi = 100)\n",
        "\n",
        "# plot matrix\n",
        "seaborn.heatmap(cm_pd, annot=True, fmt = 'g', cmap=\"YlGnBu\", cbar = False)\n",
        "\n",
        "# configure labels\n",
        "plt.title(\"Concordance matrix\")\n",
        "plt.ylabel(\"IBGE (2020)\")\n",
        "plt.xlabel(\"MapBiomas 8 (2020)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p1WYdCIvb1b"
      },
      "source": [
        "> Below, the samples are filtered considering the equality between both data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOkcGWncvpPx",
        "outputId": "5f0de610-6c72-4621-f4d6-48757aa0e7a3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate the \"water concordance matrix\" based on classes matching\n",
        "both_true = (samples_ibge['is_water'] & samples_mapbiomas['is_water'])\n",
        "both_false = (~samples_ibge['is_water'] & ~samples_mapbiomas['is_water'])\n",
        "mapbiomas_true_ibge_false = (~samples_ibge['is_water'] & samples_mapbiomas['is_water'])\n",
        "ibge_true_mapbiomas_false = (samples_ibge['is_water'] & ~samples_mapbiomas['is_water'])\n",
        "\n",
        "conditions = [mapbiomas_true_ibge_false, ibge_true_mapbiomas_false, both_true, both_false]\n",
        "choices = [2, 2, 1, 0]\n",
        "\n",
        "water_concordance = np.select(conditions, choices, default=-1)\n",
        "water_concordance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Ty5iYgvxlp"
      },
      "source": [
        "**Visualizing the filtered points in the geographical space**\n",
        "\n",
        "The map below shows the filtered samples. The blue samples represent the concordant elements. On the other hand, the yellow ones are the points where the ready did not agree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Z76XxcfnqNdA",
        "outputId": "f687883d-1e12-485e-c9f5-b86a8369ef32"
      },
      "outputs": [],
      "source": [
        "samples_df['Label'] = np.where(water_concordance == 0, \"Nao Agua\", np.where(water_concordance == 1, \"Agua\", \"Talvez Agua\"))\n",
        "samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "W-EMsOE6v2TB",
        "outputId": "9a29784a-4819-4d72-b80a-ba0844a3398e"
      },
      "outputs": [],
      "source": [
        "# create folium map\n",
        "folium_map = folium.Map(location=[-6.41, -52.35], zoom_start=13)\n",
        "\n",
        "# Google Satellite Layer\n",
        "tile = folium.TileLayer(\n",
        "        tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = False,\n",
        "        control = True\n",
        "       ).add_to(folium_map)\n",
        "\n",
        "# colors for points\n",
        "color = {\n",
        "    'Agua': '#43d9de',\n",
        "    'Nao Agua': '#008000',\n",
        "    'Talvez Agua': '#F5AD46'\n",
        "}\n",
        "# add marker to map (concordance samples)\n",
        "for index, row in samples_df.iterrows():\n",
        "    folium.CircleMarker(location=[row['geometry'].y, row['geometry'].x],\n",
        "                        fill=True,\n",
        "                        fill_color=color[row['Label']],\n",
        "                        color='black',\n",
        "                        fill_opacity=0.6,\n",
        "                        radius=9).add_to(folium_map)\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkf0VXWOHfPn"
      },
      "source": [
        "# <span style=\"color:#336699\">Web Time Series Service (WTSS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeFyguBFjiGD"
      },
      "source": [
        "## <span style=\"color:#336699\">Introduction to WTSS\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxY8GPkYjgRu"
      },
      "source": [
        "The **W**eb **T**ime **S**eries **S**ervice (WTSS) is a lightweight web service for handling time series data from remote sensing imagery. Given a location and a time interval you can retrieve the according time series as a list of real values.\n",
        "\n",
        "\n",
        "In WTSS a coverage is a three dimensional array associated to spatial and temporal reference systems.\n",
        "\n",
        "WTSS is based on three operations:\n",
        "\n",
        "- ``list_coverages``: returns the list of all available coverages in the service.\n",
        "\n",
        "- ``describe_coverage``: returns the metadata of a given coverage.\n",
        "\n",
        "- ``time_series``: query the database for the list of values for a given location and time interval.\n",
        "\n",
        "This Jupyter Notebook shows how to use WTSS in Python with Brazil Data Cube data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34v_0cYjyCS"
      },
      "source": [
        "This service is composed by three operations (Figure 2):\n",
        "\n",
        "<div align=\"center\">\n",
        "    <figcaption><strong>Figure 2</strong> - WTSS Operations</figcaption>\n",
        "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wtss/wtss-operations.png?raw=true\" align=\"center\" width=\"768\"/>\n",
        "    <br>\n",
        "    <strong>Source</strong>: Adapted from <i>et al.</i> (2017)\n",
        "</div>\n",
        "\n",
        "- ``list_coverages``: Lists the available *coverages* names on the service;\n",
        "\n",
        "- ``describe_coverage``: recovers the metadata of a *coverage*;\n",
        "\n",
        "- ``time_series``: Extracts time series from a *coverage* given a location on time and space.\n",
        "\n",
        "> The complete description of the input and output formats of each operation are detailed in [WTSS OpenAPI 3.0 specification](https://github.com/brazil-data-cube/wtss-spec).\n",
        "\n",
        "> You can also consult the WTSS clients for [Python](https://github.com/brazil-data-cube/wtss.py) and [R](https://github.com/e-sensing/Rwtss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fg40HS-j3ww"
      },
      "source": [
        "Python Client API\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaqWFJ0Vj4RN",
        "outputId": "be8fbc8e-5528-453c-e027-b3b66cd8dbd8"
      },
      "outputs": [],
      "source": [
        "!pip install wtss==2.0.0a3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW6_Xdaaj_hE",
        "outputId": "37816cb3-8c2a-408b-9075-4886fb7de133"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZFqb5mXkGqX"
      },
      "source": [
        "Connect to WTSS server\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "yQS7_bRikFtM",
        "outputId": "b60060d4-ab14-4336-e23b-5c7c19d9daf3"
      },
      "outputs": [],
      "source": [
        "from wtss import WTSS\n",
        "import os\n",
        "\n",
        "service = WTSS('https://data.inpe.br/bdc/wtss/v4/')\n",
        "service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYv6RxSukaau"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wtss/list-coverages.png?raw=true\" align=\"right\" width=\"220\"/>\n",
        "\n",
        "Listing the Available Data Products\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "The object `service` allows to list the available coverages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prEx0zLfkbwS",
        "outputId": "d27b8a95-ca54-46dc-aff5-24d98b19ee4b"
      },
      "outputs": [],
      "source": [
        "service.coverages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4-oYUbZkgmN"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wtss/time-series.png?raw=true\" align=\"right\" width=\"220\"/>\n",
        "\n",
        "Retrieving the Time Series\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "In order to retrieve the time series for attributes `red` and `nir`, in the location of `latitude -12` and `longitude -54` from `January 1st, 2019` to `December 31st, 2019`, use the `ts` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "Ajkze8E_lCJH",
        "outputId": "2ff98891-18d9-4551-ddb8-a060e93ac4e1"
      },
      "outputs": [],
      "source": [
        "coverage = service['S2-16D-2']\n",
        "coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSBIfcnpkhs2"
      },
      "outputs": [],
      "source": [
        "time_series = coverage.ts(attributes=(['B04', 'B08']),\n",
        "                 latitude=-9.41866, longitude=-61.46103,\n",
        "                 start_date='2019-01-01', end_date='2019-12-31')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "pz5Jo93ikqdt",
        "outputId": "5c768493-2ed1-497c-9fda-dcad56a343ef"
      },
      "outputs": [],
      "source": [
        "time_series.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "wiLfCzqqZQWJ",
        "outputId": "ff895739-edc3-4385-a40d-263f910466b5"
      },
      "outputs": [],
      "source": [
        "import shapely.geometry\n",
        "\n",
        "timeseries = coverage.ts(attributes=(['NDVI']),\n",
        "                         geom=shapely.geometry.box(-59.60, -5.69, -59.59, -5.68),\n",
        "                         start_datetime=\"2020-01-01\", end_datetime=\"2022-12-31\")\n",
        "timeseries.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xb1UIH46CTt"
      },
      "source": [
        "## Extracting Time Series for samples and for classification\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHXul0Pmexcz"
      },
      "source": [
        "Now let's suppose we want to classify the following area:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "t8fkmRk28aTD",
        "outputId": "36388dfe-fca3-4288-9436-617cb45a0618"
      },
      "outputs": [],
      "source": [
        "bbox = [-6.43, -52.3625, -6.425, -52.3575]\n",
        "center_lat = (bbox[0] + bbox[2]) / 2\n",
        "center_lon = (bbox[1] + bbox[3]) / 2\n",
        "\n",
        "# Create a folium map centered around the geographic area of interest\n",
        "folium.Rectangle(\n",
        "    bounds=[[bbox[0], bbox[1]], [bbox[2], bbox[3]]],\n",
        "    color='blue',\n",
        "    fill=True,\n",
        "    fill_opacity=0.2\n",
        ").add_to(folium_map)\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT3yCdnLe3fU"
      },
      "source": [
        "We can extract all time series within its bounding box using WTSS and use it as a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "9VWOIzSNTEB3",
        "outputId": "f39b76e3-539f-4cef-e5cb-b94bcdc45cc2"
      },
      "outputs": [],
      "source": [
        "import shapely.geometry\n",
        "\n",
        "attributes = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'EVI', 'NDVI', 'NBR']\n",
        "bands_and_cloudmask = attributes + ['SCL']\n",
        "\n",
        "study_area_ts = coverage.ts(attributes=(bands_and_cloudmask),\n",
        "                          geom=shapely.geometry.box(bbox[1], bbox[0], bbox[3], bbox[2]),\n",
        "                          start_date='2022-01-01', end_date='2022-12-31')\n",
        "study_area_df = study_area_ts.df()\n",
        "study_area_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uQlMz9GysH3"
      },
      "source": [
        "We can improve how the data is organized in the dataframe.\n",
        "> ⚠️ **Atention:** This will be improved in the next version of WTSS!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtXr4XU4fzX9"
      },
      "outputs": [],
      "source": [
        "def organize_df(my_df):\n",
        "    #Move attributes to columns\n",
        "    df_pivot = my_df.pivot_table(index=['geometry', 'datetime'],\n",
        "                           columns='attribute',\n",
        "                           values='value').reset_index()\n",
        "    #Group by points\n",
        "    df_grouped = df_pivot.groupby('geometry').agg(lambda x: list(x)).reset_index()\n",
        "    #Transform cells to numpy.arrays or pandas.Series\n",
        "    columns_to_convert = df_grouped.columns.difference(['geometry'])\n",
        "    df_grouped[columns_to_convert] = df_grouped[columns_to_convert].applymap(pd.Series)\n",
        "    return df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tb5zXIfHgQ0Z",
        "outputId": "fd5738a1-cb6c-45cd-99c9-6c164c7c09b5"
      },
      "outputs": [],
      "source": [
        "study_area_df = organize_df(study_area_df)\n",
        "study_area_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoCll9GXkYIb"
      },
      "source": [
        "We can see that we have time series for a total of points equals to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLAXCV8qkfsL",
        "outputId": "7d055cf4-dc9c-4f54-a283-1564e54aadb5"
      },
      "outputs": [],
      "source": [
        "len(study_area_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946QEOd1G_j8"
      },
      "source": [
        "# Applications\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEHzUyR1wWSe"
      },
      "source": [
        "## Calculating a spectral index\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lkVfxeYgtPu"
      },
      "source": [
        "Now suppose we want to generate a new spectral index for our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sggS4ZSXwu5V",
        "outputId": "0778d094-bda1-40ce-d04c-c8b8d1c42b23"
      },
      "outputs": [],
      "source": [
        "study_area_df['NDWI'] = ((study_area_df['B03'] - study_area_df['B05']) / (study_area_df['B03'] + study_area_df['B05']))\n",
        "study_area_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ZtHygglcjS"
      },
      "source": [
        "## Removing clouds from Time Series\n",
        "<hr style=\"border:1px solid #0077b9;\">\n",
        "\n",
        "When we work with satellite image time series there are external influences that can bring noise to the values of the series, distorting them. A common example of noise is the observation of clouds, which change the spectral behavior of targets, making them often unidentifiable. In Figure 1, images from the Sentinel-2/MSI sensor satellite of the `20LKP` tile are shown, where it is possible to observe that, for the same region, in close dates, the presence of clouds.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <figcaption><strong>Figure 1</strong> - Imagens Sentinel-2/MSI (Tile 20LKP) com influência de Nuvem</figcaption>\n",
        "    <img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/wtss/sentinel-2-clouds.png?raw=true\" align=\"center\" width=\"620rem\"/>\n",
        "    <br>\n",
        "    <strong>Fonte</strong>: Simoes <i>et al.</i> (2021)\n",
        "</div>\n",
        "\n",
        "In the context of satellite image time series, a possible approach for the treatment of this type of noise is to replace the time series points that have cloud influence by interpolated values.\n",
        "\n",
        "For this, it is necessary to have data available to identify the influence of the cloud at each point in the time series. When considering the BDC data products, this approach is possible since the data cubes are produced with masks that identify, *pixel* to *pixel*, the cloud influence on the images.\n",
        "\n",
        "Therefore, in this second example, we will use the cloud mask provided in the BDC data products, together with the linear interpolation technique, to remove the cloud influence from the time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjGrCirPlfxe"
      },
      "source": [
        "In this metadata, we can see the attribute `SCL`, which stands for \"Scene Classification Layer\" and uses the following values:\n",
        "\n",
        "- `0`: no data;\n",
        "- `1`: saturated or defective;\n",
        "- `2`: dark area pixels;\n",
        "- `3`: cloud shadow;\n",
        "- `4`: vegetation;\n",
        "- `5`: not vegetated;\n",
        "- `6`: water;\n",
        "- `7`: unclassified;\n",
        "- `8`: cloud medium probability;\n",
        "- `9`: cloud high probability;\n",
        "- `10`: thin cirrus;\n",
        "- `11`: snow;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1XbfA8Oz9yJ"
      },
      "source": [
        "Let's make a copy of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgIlozRJkRy_"
      },
      "outputs": [],
      "source": [
        "interpolated_df = study_area_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftVQn-HolxE-"
      },
      "source": [
        "Now, we need to adopt some strategy to make the values that have cloud influence be marked as `nan` in the dataset.\n",
        "\n",
        "As the values of both attributes are stored in a `numpy.array`, we can then use the features of [numpy](https://numpy.org/), to manipulate this data.\n",
        "\n",
        "Considering this, we are going to adopt a strategy in which we will apply a transformation to the `SCL` data, so that every position in the array that represents cloud influence will be converted to `nan`, while the other values will be converted to `1`. With this approach, we will be able to multiply the array resulting from the transformation, with the data of other attributes, which will make the positions with cloud influence become `nan` as well.\n",
        "\n",
        "To be clear, let's perform this operation first with a single example point:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODT-P2fu0BqO"
      },
      "source": [
        "and extract a single point to demonstrate how we will interpolate dates containing clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Hy7rpGYMCC6O",
        "outputId": "97a8e030-69ec-46a6-9cae-17c8937c3d20"
      },
      "outputs": [],
      "source": [
        "example_point = interpolated_df.loc[0, ['SCL', 'NDVI']]\n",
        "example_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "08CFnn5_Cmrl",
        "outputId": "331516ab-293c-4cc5-8317-7b2d8cbf54b1"
      },
      "outputs": [],
      "source": [
        "example_point['SCL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96N3ozYK4F-j",
        "outputId": "3b89e4da-1f53-49ca-e36a-1daeb35ed4f6"
      },
      "outputs": [],
      "source": [
        "np.unique(example_point['SCL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3mDYx-DloBZ"
      },
      "source": [
        "Note that, among the values returned, there are *pixels* that have values `7`, `8`, `9` or `10`. This indicates that certain values have cloud and may be removed.\n",
        "\n",
        "To present the example of interpolation, let's make use of the `NDVI` attribute. So, first let's do a new time series extraction, considering now both attributes, `NDVI` and `SCL`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn6PvsEJDH9p",
        "outputId": "3ca75b27-4071-45e4-c5e8-7f26c8120968"
      },
      "outputs": [],
      "source": [
        "values_to_fill = [0,1,2,3,7,8,9,10]\n",
        "positions_to_mask = np.where(np.isin(example_point['SCL'], values_to_fill), np.nan, 1)\n",
        "positions_to_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1vjlwI4-Go"
      },
      "source": [
        "Note that now, there are some `nan` values in the `SCL` array, while all others are equal to `1`. By multiplying this mask array, with the `NDVI` array, we have the following effect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "K7XZFBlSFgJS",
        "outputId": "35cca38b-0be4-47aa-828f-75a7f7f7193e"
      },
      "outputs": [],
      "source": [
        "example_point_masked= example_point['NDVI'] * positions_to_mask\n",
        "example_point_masked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkDzDSTz0oTl"
      },
      "source": [
        "And this is the result of the time series with the cloud values interpolated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4SzSG32l4Qh"
      },
      "source": [
        "Note that among the values of `NDVI` in the time series, some values were transformed into `nan`. Now, we can perform the interpolation of our time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeiC43KrmALk"
      },
      "source": [
        "Now, using the `interpolate` method, available in `pandas.Series`, let's interpolate our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "YBRF4CsSHKZJ",
        "outputId": "f5983d39-c447-423b-f011-b26a1c233f84"
      },
      "outputs": [],
      "source": [
        "example_interpolated = example_point_masked.interpolate()\n",
        "example_interpolated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akCln_2rmCJ3"
      },
      "source": [
        "The interpolation result was saved in the `example_interpolated` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv51HNZR5Aon",
        "outputId": "c87f6ed4-c7ae-4760-dfe6-d3b713849f09"
      },
      "outputs": [],
      "source": [
        "print(example_point['NDVI'].values)\n",
        "print(example_interpolated.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYy3sbGlmM-V"
      },
      "source": [
        "Done! We interpolate our data. To visualize the result of this operation and the difference caused by this change in the series, below, let's visualize the original series and the interpolated series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "pzZQtioF4PjH",
        "outputId": "c2125d65-4c28-47a8-c71e-dd41bcc0f782"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi = 120)\n",
        "\n",
        "plt.plot(example_point['NDVI'].values, color='gray', linestyle='dashed', label = 'Original')\n",
        "plt.plot(example_interpolated.values, color='blue', label = 'Interpolated')\n",
        "\n",
        "scl_colors = {\n",
        "    '0': 'black',  #Nodata\n",
        "    '1': 'red',    #Saturated or defective\n",
        "    '2': 'gray',   #Dark area pixels\n",
        "    '3': 'brown',  #Cloud Shadow\n",
        "    '4': 'green',  #Vegetation\n",
        "    '5': 'yellow', #Not Vegetated\n",
        "    '6': 'blue',   #Water\n",
        "    '7': 'red',    #Unclassified\n",
        "    '8': 'white',  #Cloud Medium Probability\n",
        "    '9': 'white',  #Cloud High Probability\n",
        "    '10': 'cyan',  #Thin Cirrus\n",
        "    '11': 'pink'   #Snow\n",
        "}\n",
        "\n",
        "# Add colored markers for each point based on SCL\n",
        "for idx, scl_value in enumerate(example_point['SCL']):\n",
        "    color = scl_colors[str(int(scl_value))]\n",
        "    plt.scatter(idx, example_point['NDVI'].values[idx], edgecolor='black', color=color, s=50, zorder=5)\n",
        "\n",
        "plt.title('Comparison of Time Series with and without interpolation of cloud areas')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8XkxgPP1FNT"
      },
      "source": [
        "Now let's do this for all time series in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "NrlOdUZUWuUe",
        "outputId": "ef3ba64a-7ee5-40bc-cb3e-ec7a6d9f23ad"
      },
      "outputs": [],
      "source": [
        "def interpolate_clouds(df_row):\n",
        "    for band in attributes:\n",
        "        temp_array = df_row[band]\n",
        "        positions_to_mask = np.where(np.isin(df_row['SCL'], values_to_fill), np.nan, 1)\n",
        "        temp_array = temp_array * positions_to_mask\n",
        "        df_row[band] = temp_array.interpolate()  # Use the row to assign interpolated values\n",
        "    return df_row\n",
        "\n",
        "interpolated_df = interpolated_df.apply(interpolate_clouds, axis=1)\n",
        "interpolated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzDInMm8-dT3"
      },
      "source": [
        "## Time Series Classification\n",
        "<hr style=\"border:1px solid #0077b9;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE6xbfokluMk"
      },
      "source": [
        "Now, let's perform a classification using the data we obtained and prepared.\n",
        "> ⚠️ **Atention:** This is a didatic example. Some classification approaches will prefer the input data without the interpolation we did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITwxQTt3nIwA"
      },
      "source": [
        "First let's prepare our samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dHakV7DZXGmF",
        "outputId": "8fab1bf9-6108-45d9-9c77-35f0cd64f768"
      },
      "outputs": [],
      "source": [
        "attributes = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'EVI', 'NDVI', 'NBR']\n",
        "bands_and_cloudmask = attributes + ['SCL']\n",
        "\n",
        "### You can use MultiPoint in WTSS to extract the time series as, but this will be improved in the next version, due to timeout\n",
        "# multipoint = MultiPoint(samples_df['geometry'].tolist())\n",
        "# time_series = coverage.ts(attributes=(bands_and_cloudmask),\n",
        "#                  geom=multipoint,\n",
        "#                  start_date='2022-01-01', end_date='2022-12-31')\n",
        "\n",
        "\n",
        "start_df = True\n",
        "i=1\n",
        "for index, row in samples_df.iterrows():\n",
        "    print(f\"{i} of {len(samples_df)} ..\")\n",
        "    i+=1\n",
        "    latitude, longitude = row['geometry'].y, row['geometry'].x\n",
        "    sample_time_series = coverage.ts(attributes=(bands_and_cloudmask),\n",
        "                                     latitude=float(latitude), longitude=float(longitude),\n",
        "                                     start_date='2022-01-01', end_date='2022-12-31')\n",
        "    temp_df = sample_time_series.df()\n",
        "\n",
        "    #Move attributes to columns\n",
        "    df_pivot = temp_df.pivot_table(index=['geometry', 'datetime'],\n",
        "                           columns='attribute',\n",
        "                           values='value').reset_index()\n",
        "    #Group by points\n",
        "    df_grouped = df_pivot.groupby('geometry').agg(lambda x: list(x)).reset_index()\n",
        "    #Transform cells to numpy.arrays or pandas.Series\n",
        "    columns_to_convert = df_grouped.columns.difference(['geometry'])\n",
        "    df_grouped[columns_to_convert] = df_grouped[columns_to_convert].applymap(pd.Series)\n",
        "    df_grouped['Label'] = row['Label']\n",
        "    if start_df:\n",
        "        ts_df = df_grouped.copy()\n",
        "        start_df = False\n",
        "    else:\n",
        "        ts_df = pd.concat([ts_df, df_grouped], ignore_index=True)\n",
        "ts_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQAzaKinrQB"
      },
      "source": [
        "Let's calculate the same index, and try to remove clouds of our time series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIiY508nd0M"
      },
      "outputs": [],
      "source": [
        "ts_df['NDWI'] = ((ts_df['B03'] - ts_df['B05']) / (ts_df['B03'] + ts_df['B05']))\n",
        "ts_df = ts_df.apply(interpolate_clouds, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w19oSbHRn1gt"
      },
      "source": [
        "Let's remove from our samples, points of Water that had differences according to IBGE and mapbiomas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03cc5M3woNrI",
        "outputId": "9353b1cb-1cb6-4ae3-9302-126cd06e2325"
      },
      "outputs": [],
      "source": [
        "print(len(ts_df))\n",
        "ts_df = ts_df[ts_df['Label'] != \"Talvez Agua\"].reset_index(drop=True)\n",
        "print(len(ts_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVADdw3xpR8E"
      },
      "source": [
        "Now we have our samples Dataframe, called `ts_df` and the Dataframe of time series we want to classify, the `study_area_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "L02U4sFb-fsk",
        "outputId": "5bde37db-f4e1-4835-e633-c7b47165f248"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def series_to_features(df):\n",
        "    feature_list = []\n",
        "    for col in df.columns:\n",
        "        # Stack values\n",
        "        col_values = df[col].apply(lambda series: series.values if isinstance(series, pd.Series) else series)\n",
        "        feature_list.append(np.vstack(col_values))\n",
        "\n",
        "    features = np.hstack(feature_list)\n",
        "    return features\n",
        "\n",
        "feature_columns = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A', 'EVI', 'NBR', 'NDVI', 'SCL']\n",
        "\n",
        "X_samples = series_to_features(ts_df[feature_columns])\n",
        "y_samples = ts_df['Label']\n",
        "\n",
        "X_to_class = series_to_features(study_area_df[feature_columns])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_samples, y_samples, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline.fit(X_train, y_train) #Train model\n",
        "\n",
        "predicted_labels = pipeline.predict(X_to_class) #Use model to classify\n",
        "study_area_df['Label'] = predicted_labels\n",
        "\n",
        "study_area_df['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "lZIs66y-9GcC",
        "outputId": "6abe5aaa-517f-46af-d6d1-f884aaa16cf9"
      },
      "outputs": [],
      "source": [
        "f = folium.Figure(width=1000, height=300) # Restrict figure size\n",
        "\n",
        "folium_map = folium.Map(location=[-6.4275, -52.36], zoom_start=17.2)\n",
        "\n",
        "tile = folium.TileLayer(\n",
        "        tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\",\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = False,\n",
        "        control = True\n",
        "       ).add_to(folium_map)\n",
        "\n",
        "for _, row in study_area_df.iterrows():\n",
        "    point = row['geometry']\n",
        "\n",
        "    marker_color = 'blue' if row['Label'] == \"Agua\" else 'gray'\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=(point.y, point.x),\n",
        "        radius=2,\n",
        "        color=marker_color,\n",
        "        fill=True,\n",
        "        fill_color=marker_color,\n",
        "        fill_opacity=0.5,\n",
        "        popup='Sample'\n",
        "    ).add_to(folium_map)\n",
        "folium_map"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

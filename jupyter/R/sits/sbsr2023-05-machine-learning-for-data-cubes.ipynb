{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src= \"https://raw.githubusercontent.com/e-sensing/sits/master/inst/extdata/sticker/sits_sticker.png\" align=\"left\" width=\"64\"/>\n<img src=\"https://raw.githubusercontent.com/brazil-data-cube/code-gallery/master/img/logo-bdc.png\" align=\"right\" width=\"64\" />\n\n# <span align=\"center\" style=\"color:#336699\" >Machine Learning for Data Cubes in <b>sits</b></span>\n<hr style=\"border:2px solid #0077b9;\">\n\n<div style=\"text-align: left;\">\n    <a href=\"https://nbviewer.jupyter.org/github/brazil-data-cube/code-gallery/blob/master/jupyter/R/sits/sits-timeseries-classification.ipynb\"><img src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" align=\"center\"/></a>\n</div>\n\n<br/>\n\n<div style=\"text-align: center;font-size: 90%;\">\n    Rolf Simoes<sup><a href=\"https://orcid.org/0000-0003-0953-4132\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Felipe Souza<sup><a href=\"https://orcid.org/0000-0001-7534-0219\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>, Gilberto Camara<sup><a href=\"https://orcid.org/0000-0001-7534-0219\"><i class=\"fab fa-lg fa-orcid\" style=\"color: #a6ce39\"></i></a></sup>\n    <br/><br/>\n    Earth Observation and Geoinformatics Division, National Institute for Space Research (INPE)\n    <br/>\n    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n    <br/><br/>\n    Contact: <a href=\"mailto:brazildatacube@inpe.br\">brazildatacube@inpe.br</a>\n    <br/><br/>\n    Last Update: September 02, 2022\n</div>\n\n<br/>\n\n<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;\">\n<b>Abstract</b><br>\n    <b>sits</b> is an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes.This Jupyter Notebook shows how to work with data cubes in `sits`. This notebook corresponds to the chapter \"Machine Learning for Data Cubes\" in the <a href=\"https://e-sensing.github.io/sitsbook/\" target=\"_blank\"> book on the SITS package</a>.\n</div>    \n\n<br/>\n<div style=\"text-align: justify;  margin-left: 15%; margin-right: 15%;font-size: 100%; border-style: solid; border-color: #0077b9; border-width: 1px; padding: 5px;\">\n    <b>For a comprehensive sits overview and discussion, please, refer to the following gihub book:</b>\n    <div style=\"margin-left: 10px; margin-right: 10px\">\n    Simoes, R; Camara, G.; Souza, F.; Santos, L.; Andrade, P.; Carvalho, A.; Ferreira, K.; Queiroz, G. <a href=\"https://e-sensing.github.io/sitsbook/\" target=\"_blank\">sits: Satellite Image Time Series</a>.\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Setup\n<hr style=\"border:1px solid #0077b9;\">","metadata":{}},{"cell_type":"code","source":"# install sits and sitsdata packages\nsystem(\"cp -u -R ../input/sits-bundle/sits-bundle/* /usr/local/lib/R/site-library/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(sits)\nlibrary(sitsdata)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine learning and deep learning models available on sits\n<hr style=\"border:1px solid #0077b9;\">","metadata":{}},{"cell_type":"markdown","source":"The following machine learning methods are available in sits:\n\n- Random forests (`sits_rfor()`)\n- Support vector machines (`sits_svm()`)\n- Extreme gradient boosting (`sits_xgboost()`)\n- Multilayer perceptrons (`sits_mlp()`)\n- Deep Residual Networks (`sits_resnet()`)\n- 1D convolutional neural networks (`sits_tempcnn()`)\n- Temporal attention encoders (`sits_tae()`)\n- Lightweight temporal attention encoders (`sits_lighttae()`)","metadata":{}},{"cell_type":"markdown","source":"## Visualizing Sample Patterns\n<hr style=\"border:1px solid #0077b9;\">","metadata":{}},{"cell_type":"code","source":"options(repr.plot.width = 12, repr.plot.height = 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimate the patterns for each class and plot them\nmt_patterns <- sits_patterns(samples_matogrosso_mod13q1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(mt_patterns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random forests\n<hr style=\"border:1px solid #0077b9;\">","metadata":{}},{"cell_type":"markdown","source":"<p text-align=\"center\">\nThe random forests model uses decision trees as its base model with refinements. When building the decision trees, each time a split in a tree is considered, a random sample of m features is chosen as split candidates from the full set of n features of the samples. Each of these features is then tested; the one maximizing the decrease in a purity measure is used to build the trees. This criterion is used to identify relevant features and to perform variable selection. This decreases the correlation among trees and improves prediction performance. Classification performance depends on the number of trees in the forest as well as the number of features randomly selected at each node.\n</p>\n\n\n<figure>\n  <img src=\"https://e-sensing.github.io/sitsbook/images/random_forest.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 50%;\" />\n  <figcaption style=\"text-align: center;\"> Random forests algorithm (source: Venkata Jagannath in Wikipedia - licenced as CC-BY-SA 4.0.)</figcaption>\n</figure>","metadata":{}},{"cell_type":"code","source":"# Train the Mato Grosso samples with Random Forests model.\nrfor_model <- sits_train(\n  samples   = samples_matogrosso_mod13q1,\n  ml_method = sits_rfor(num_trees = 100)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options(repr.plot.width = 10, repr.plot.height = 6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the most important variables of the model\nplot(rfor_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classify using Random Forest model and plot the result\npoint_class <- sits_classify(\n  data     = point_mt_mod13q1,\n  ml_model = rfor_model\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(point_class, bands = c(\"NDVI\", \"EVI\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Temporal Convolutional Neural Network (TempCNN)\n<hr style=\"border:1px solid #0077b9;\">","metadata":{}},{"cell_type":"markdown","source":"<p text-align=\"center\">\nConvolutional neural networks (CNN) are a variety of deep learning methods where a convolution filter (sliding window) is applied to the input data. In the case of time series, a 1D CNN works by applying a moving window to the series. Using convolution filters is a way to incorporate temporal autocorrelation information in the classification. The result of the convolution is another time series. Russwurm and Körner state that the use of 1D-CNN for time series classification improves on the use of multilayer perceptrons, since the classifier is able to represent temporal relationships; also, the convolution window makes the classifier more robust to moderate noise, e.g. intermittent presence of clouds.\n</p>\n\n\n<figure>\n  <img src=\"https://e-sensing.github.io/sitsbook/images/tempcnn.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 70%;\" />\n  <figcaption style=\"text-align: center;\">Structure of tempCNN architecture (source: Pelletier et al.(2019))</figcaption>\n</figure>","metadata":{}},{"cell_type":"code","source":"# train using TempCNN\ntempcnn_model <- sits_train(\n  samples   = samples_matogrosso_mod13q1,\n  ml_method = sits_tempcnn(\n    optimizer         = torchopt::optim_adamw,\n    cnn_layers        = c(128, 128, 128),\n    cnn_kernels       = c(7, 7, 7),\n    cnn_dropout_rates = c(0.2, 0.2, 0.2),\n    epochs            = 20,\n    batch_size        = 64,\n    validation_split  = 0.2\n  )\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show training evolution\nplot(tempcnn_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classify using TempCNN model and plot the result\npoint_class <- sits_classify(\n    data     = point_mt_mod13q1,\n    ml_model = tempcnn_model\n)\n\nplot(point_class, bands = c(\"NDVI\", \"EVI\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}